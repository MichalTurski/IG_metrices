{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ranges'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f91d7ca6edfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mranges\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ranges'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import ranges\n",
    "from collections import defaultdict\n",
    "\n",
    "from MAE_parser import MAE_parser\n",
    "\n",
    "\n",
    "def df_from_dir(dir_path):\n",
    "    mae_parser = MAE_parser()\n",
    "    xml_files = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    xml_files.sort()\n",
    "    dfs_list = []\n",
    "    for file_id, file_name in enumerate(xml_files):\n",
    "        df = mae_parser.parse_file(join(dir_path, file_name))\n",
    "        df['file_id'] = file_id\n",
    "        dfs_list.append(df)\n",
    "    return pd.concat(dfs_list, sort=False)\n",
    "\n",
    "\n",
    "def clean_dfs(gold_df, app_df):\n",
    "    unknown_columns = [gold_df_col for gold_df_col in gold_df.columns if gold_df_col not in app_df.columns]\n",
    "    gold_df.drop(columns=unknown_columns, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "gold_df = df_from_dir(\"./gold_anots\")\n",
    "app_df = df_from_dir(\"./app_anots\")\n",
    "clean_dfs(gold_df, app_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns length of ranges within range set\n",
    "def get_range_set_length(range_set):\n",
    "    return sum((r.length() for r in range_set))\n",
    "\n",
    "# creates range set of ranges based on tag spans in given data frame view\n",
    "def get_ranges(view_df):\n",
    "    range_set = ranges.RangeSet()\n",
    "    for row in view_df.itertuples(index=False):\n",
    "        range_set.update(ranges.Range(row.begin, row.end))\n",
    "    return range_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates coverage for given file and category\n",
    "# correct_coverage_length is length of all spans that were annotated by the same category in given file\n",
    "# wrong_coverage_length is length of all spans that application annotated by given category and GS annotated them with another category\n",
    "# gs_coverage_length is length of all spans that gs annotated by given category\n",
    "# app_coverage_length is length of all spans that application annotated by given category\n",
    "\n",
    "def calculate_coverage(file_id, category, gold_df, app_df):\n",
    "    app_ranges = get_ranges(app_df[(app_df['file_id'] == file_id) & (app_df['category'] == category)])\n",
    "    gold_ranges = get_ranges(gold_df[(gold_df['file_id'] == file_id) & (gold_df['category'] == category)])\n",
    "    correct_coverage = get_range_set_length(app_ranges.intersection(gold_ranges)) / get_range_set_length(gold_ranges)\n",
    "    wrong_coverage = get_range_set_length(app_ranges.difference(app_ranges.intersection(gold_ranges))) / get_range_set_length(gold_ranges)\n",
    "    return {'file_id': file_id, \n",
    "            'category': category, \n",
    "            'correct_coverage_length': get_range_set_length(app_ranges.intersection(gold_ranges)), \n",
    "            'wrong_coverage_length': get_range_set_length(app_ranges.difference(app_ranges.intersection(gold_ranges))),\n",
    "            'gs_coverage_length': get_range_set_length(gold_ranges),\n",
    "            'app_coverage_length': get_range_set_length(app_ranges),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = app_df['file_id'].unique()\n",
    "categories = app_df['category'].unique()\n",
    "\n",
    "tag_coverage = []\n",
    "for file_id in file_ids:\n",
    "    for category in categories:\n",
    "        tag_cov = calculate_coverage(file_id, category, gold_df, app_df)\n",
    "        tag_coverage.append(tag_cov)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies percentage coverage calculation\n",
    "def apply_percentage_calculations(df):\n",
    "    df['app_coverage_perc'] = df.apply(lambda row: None if row.app_coverage_length == 0 else round(row.correct_coverage_length * 100 / row.app_coverage_length, 2), axis=1)\n",
    "    df['gs_coverage_perc'] = df.apply(lambda row: None if row.gs_coverage_length == 0 else round(row.correct_coverage_length * 100 / row.gs_coverage_length, 2), axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = pd.DataFrame(tag_coverage)\n",
    "general_summary_df = coverage_df.groupby('category').sum().drop(columns=['file_id'])\n",
    "apply_percentage_calculations(general_summary_df)\n",
    "\n",
    "file_summary_df = coverage_df.groupby(['file_id', 'category']).sum()\n",
    "apply_percentage_calculations(file_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_summary_df.to_csv('general_summary.csv')\n",
    "file_summary_df.to_csv('file_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN ISSUE:\n",
    "\n",
    "# ig_annotator annotates preprocessed text that differs from original\n",
    "# this tags refer to the same part of text\n",
    "# APP annotation: <aCtor id=\"aC7\" spans=\"16265~16344\" text=\"6 Ogólnouniwersytecką jednostkę organizacyjną, o której mowa w ust 1 pkt 2 i 4,\" />\n",
    "# GS annotation: <aCtor id=\"aC49\" spans=\"16621~16700\" text=\"Ogólnouniwersytecką jednostkę organizacyjną, o której mowa w ust. 1  pkt 2 i 4,\"  />\n",
    "\n",
    "# It's visible that this annotation of tag should get high score rating because application added just 2 characters too much on the beggining. However it gets zero score in current metric.\n",
    "# Current metric is based on span coverage. Application get scoring based on spans that annotated the same in GS fle and APP file. \n",
    "# There was also idea to do metric based on text similiarity but without knowledge about location of given text in file (without spans) \n",
    "# there could be (and there are) multiple tags that annotates the same words but in different contexts, eg: deontics.\n",
    "\n",
    "# Main goal of this metric (based on span coverage) was that if application annotation differs by few characters on end or start it will achieve high score anyway.\n",
    "# However due to spans being missplaced (ig-annotator introduced new lines) and GS (created in MAE has double ) this metric kinda sucks.\n",
    "# Anyway I think it will be near to impossible to create sensable metric based on two files whose baseline (same spans represent another texts) in skewed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}