{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import ranges\n",
    "from collections import defaultdict\n",
    "\n",
    "from MAE_parser import MAE_parser\n",
    "\n",
    "\n",
    "def df_from_dir(dir_path):\n",
    "    mae_parser = MAE_parser()\n",
    "    xml_files = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    xml_files.sort()\n",
    "    dfs_list = []\n",
    "    for file_id, file_name in enumerate(xml_files):\n",
    "        df = mae_parser.parse_file(join(dir_path, file_name))\n",
    "        df['file_id'] = file_id\n",
    "        dfs_list.append(df)\n",
    "    return pd.concat(dfs_list, sort=False)\n",
    "\n",
    "\n",
    "def clean_dfs(gold_df, app_df):\n",
    "    unknown_columns = [gold_df_col for gold_df_col in gold_df.columns if gold_df_col not in app_df.columns]\n",
    "    gold_df.drop(columns=unknown_columns, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "gold_df = df_from_dir(\"./gold_anots\")\n",
    "app_df = df_from_dir(\"./app_anots\")\n",
    "# clean_dfs(gold_df, app_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# returns length of ranges within range set\n",
    "def get_range_set_length(range_set):\n",
    "    return sum((r.length() for r in range_set))\n",
    "\n",
    "# creates range set of ranges based on tag spans in given data frame view\n",
    "def get_ranges(view_df):\n",
    "    range_set = ranges.RangeSet()\n",
    "    for row in view_df.itertuples(index=False):\n",
    "        range_set.update(ranges.Range(row.begin, row.end))\n",
    "    return range_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   id                                               text  begin  end  \\\n",
      "0  S0                                                 1.     50   52   \n",
      "1  o0  Grupy stanowisk pracowników niebędących nauczy...     53  110   \n",
      "2  a0                                            określa    112  118   \n",
      "3  o1               regulamin organizacyjny Uniwersytetu    119  151   \n",
      "4  S1                                                 2.    155  157   \n",
      "\n",
      "    category RodzajAC  file_id @Voice @komentarz  \n",
      "0  SEPARATOR      NaN        0    NaN        NaN  \n",
      "1     oBject      NaN        0    NaN        NaN  \n",
      "2        aIm      NaN        0    NaN        NaN  \n",
      "3     oBject      NaN        0    NaN        NaN  \n",
      "4  SEPARATOR      NaN        0    NaN        NaN  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gold_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# calculates coverage for given file and category\n",
    "# correct_coverage_length is length of all spans that were annotated by the same category in given file\n",
    "# wrong_coverage_length is length of all spans that application annotated by given category and GS annotated them with another category\n",
    "# gs_coverage_length is length of all spans that gs annotated by given category\n",
    "# app_coverage_length is length of all spans that application annotated by given category\n",
    "\n",
    "def calculate_coverage(file_id, category, gold_df, app_df):\n",
    "    app_ranges = get_ranges(app_df[(app_df['file_id'] == file_id) & (app_df['category'] == category)])\n",
    "    gold_ranges = get_ranges(gold_df[(gold_df['file_id'] == file_id) & (gold_df['category'] == category)])\n",
    "    correct_coverage = get_range_set_length(app_ranges.intersection(gold_ranges)) / get_range_set_length(gold_ranges)\n",
    "    wrong_coverage = get_range_set_length(app_ranges.difference(app_ranges.intersection(gold_ranges))) / get_range_set_length(gold_ranges)\n",
    "    return {'file_id': file_id, \n",
    "            'category': category, \n",
    "            'correct_coverage_length': get_range_set_length(app_ranges.intersection(gold_ranges)), \n",
    "            'wrong_coverage_length': get_range_set_length(app_ranges.difference(app_ranges.intersection(gold_ranges))),\n",
    "            'gs_coverage_length': get_range_set_length(gold_ranges),\n",
    "            'app_coverage_length': get_range_set_length(app_ranges),\n",
    "           }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "file_ids = app_df['file_id'].unique()\n",
    "categories = app_df['category'].unique()\n",
    "\n",
    "tag_coverage = []\n",
    "for file_id in file_ids:\n",
    "    for category in categories:\n",
    "        tag_cov = calculate_coverage(file_id, category, gold_df, app_df)\n",
    "        tag_coverage.append(tag_cov)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# applies percentage coverage calculation\n",
    "def apply_percentage_calculations(df):\n",
    "    df['app_coverage_perc'] = df.apply(lambda row: None if row.app_coverage_length == 0 else round(row.correct_coverage_length * 100 / row.app_coverage_length, 2), axis=1)\n",
    "    df['gs_coverage_perc'] = df.apply(lambda row: None if row.gs_coverage_length == 0 else round(row.correct_coverage_length * 100 / row.gs_coverage_length, 2), axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "coverage_df = pd.DataFrame(tag_coverage)\n",
    "general_summary_df = coverage_df.groupby('category').sum().drop(columns=['file_id'])\n",
    "apply_percentage_calculations(general_summary_df)\n",
    "\n",
    "file_summary_df = coverage_df.groupby(['file_id', 'category']).sum()\n",
    "apply_percentage_calculations(file_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "general_summary_df.to_csv('general_summary.csv')\n",
    "file_summary_df.to_csv('file_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# MAIN ISSUE:\n",
    "\n",
    "# ig_annotator annotates preprocessed text that differs from original\n",
    "# this tags refer to the same part of text\n",
    "# APP annotation: <aCtor id=\"aC7\" spans=\"16265~16344\" text=\"6 Ogólnouniwersytecką jednostkę organizacyjną, o której mowa w ust 1 pkt 2 i 4,\" />\n",
    "# GS annotation: <aCtor id=\"aC49\" spans=\"16621~16700\" text=\"Ogólnouniwersytecką jednostkę organizacyjną, o której mowa w ust. 1  pkt 2 i 4,\"  />\n",
    "\n",
    "# It's visible that this annotation of tag should get high score rating because application added just 2 characters too much on the beggining. However it gets zero score in current metric.\n",
    "# Current metric is based on span coverage. Application get scoring based on spans that annotated the same in GS fle and APP file. \n",
    "# There was also idea to do metric based on text similiarity but without knowledge about location of given text in file (without spans) \n",
    "# there could be (and there are) multiple tags that annotates the same words but in different contexts, eg: deontics.\n",
    "\n",
    "# Main goal of this metric (based on span coverage) was that if application annotation differs by few characters on end or start it will achieve high score anyway.\n",
    "# However due to spans being missplaced (ig-annotator introduced new lines) and GS (created in MAE has double ) this metric kinda sucks.\n",
    "# Anyway I think it will be near to impossible to create sensable metric based on two files whose baseline (same spans represent another texts) in skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}